---
permalink: esds/task_esds_r640_drive_repl.html
sidebar: sidebar
keywords: solidfire esds, replace drive, proactive drive replacement, cache drive replacement, faulty drive replacement, r640
summary: Perform this procedure if you want to proactively replace a metadata drive or a block drive in your SolidFire eSDS cluster. The Element UI Cluster > Drives page shows the drive wear information.
---
= Replace drives for Dell R640
:icons: font
:imagesdir: ../media/

[.lead]
Choose from the procedures listed here to replace a drive proactively, replace a drive after it has failed, and replace a cache drive. replace a metadata drive or a block drive in your SolidFire eSDS cluster. The Element UI *Cluster > Drives* page shows the drive wear information.

* <<Replace a drive proactively>>
* <<Replace a faulty drive>>
* <<Replace a cache drive>>

=== Replace a drive proactively

Perform this procedure if you want to proactively replace a metadata drive or a block drive in your SolidFire eSDS cluster. The Element UI *Cluster* > *Drives* page shows the drive wear information.

.What you'll need

* From the NetApp Element software UI, ensure that your cluster is in good health and there are no warnings or cluster faults. You can access the Element UI by using the management virtual IP (MVIP) address of the primary cluster node.
* Ensure that there are no active jobs running on the cluster.
* Ensure that you have familiarized yourself with all the steps.
* Ensure that you take necessary precautions to prevent electrostatic discharge (ESD) while handling drives.

.Steps

. Perform the following steps in the Element UI:
 .. In the Element UI, select *Cluster* > *Drives* > *Active*.
 .. Select the drive that you want to replace.
 .. Make a note of the serial number of the drive. This will help you locate the corresponding slot number of the drive in the Integrated Dell Remote Access Controller (iDRAC).
 .. Select *Bulk Actions* > *Remove*. After you remove the drive, the drive goes into the *Removing* state. It stays in the *Removing* state for a while, waiting for the data on the drive to be synced or redistributed to the remaining drives in the cluster. After the remove is complete, the drive moves to the *Available* state.
. Perform the following steps to locate the drive slot of the drive that you are replacing:
.. Log in to the IPMI interface of the node (iDRAC in this case).
.. Select *Storage* from the menu, and then select *Physical Disks*.
.. To find the serial number of the drive, select the *+* icon next to each PCIe SSD.
.. Match the serial number you see on the screen with what you noted in the Element UI.
.. Look for the slot number listed against the serial number.
+
This is the physical slot from which you should remove the drive.

. Now that you have identified the drive, physically remove it as follows:
.. Identify the drive slot number in the chassis.
+
The following image shows the front of the server with the slot numbering shown for each drive:
+
image::../media/esds-dell.png[Shows the front of the R640 server with the slot numbering shown for each drive.]

.. Press the button on the drive.
+
The latch comes out.
.. Physically pull the drive out from the slot.
+
NOTE: Ensure that you handle drives very carefully.
+
After you physically remove the drive, the drive state changes to *Failed* in the Element UI.
. In the Element UI, select *Cluster* > *Drives* > *Failed*.
. Select the icon under *Actions*, and then select *Remove*.
+
Now you can go ahead and install the new drive in the node.

. Make a note of the serial number of the new drive.
. Insert the replacement drive by carefully pushing the drive into the bay using the latch and closing the latch. The drive powers on when inserted correctly.
. Perform the following steps to verify the new drive details in iDRAC:
.. Log in to iDRAC.
.. Select *Maintenance* > *System Event Log*.
+
You see an event logged for the drive that you added.
.. Select *Storage* from the menu, and then select *Physical Disks*.
.. Verify that the new drive you inserted shows up in the corresponding slot in the UI.
.. To find the serial number of the drive, select the *+* icon next to each PCIe SSD.

. Add the new drive information in the `sf_sds_config.yaml` file for the node in which you replaced the drive.
+
The `sf_sds_config.yaml` file is stored in `/opt/sf/`. This file includes all the information about the drives in the node. Every time you replace a drive, you must enter the replacement drive information in this file. For more information about this file, see link:reference_esds_sf_sds_config_file.html[Contents of the sf_sds_config.yaml file^].
+
.. Establish an SSH connection to the node by using PuTTY.
.. In the PuTTY configuration window, enter node MIP in the *Host Name (or IP address)* field.
.. Select *Open*.
.. In the terminal window that opens, log in with your username and password.
.. Run the `# cat /opt/sf/sf_sds_config.yaml` command to list the contents of the file.
.. Replace the entries in the `dataDevices` or `cacheDevices` lists for the drive you replaced with the new drive information.
.. Run `# systemctl start solidfire-update-drives`.
+
You see the Bash prompt after this command runs. You should go to the Element UI after this to add the drive to the cluster. The Element UI shows an alert for a new drive that is available.

. Select *Cluster* > *Drives* > *Available*.
+
You see the serial number of the new drive that you installed.

. Select the icon under *Actions*, and then select *Add*.
. Refresh the Element UI after the block sync job completes. You see that the alert about the drive available has cleared if you access the *Running Tasks* page from the *Reporting* tab of the Element UI.

=== Replace a faulty drive

If your SolidFire eSDS cluster has a faulty drive, the Element UI displays an alert. Before you remove the drive from the cluster, verify the reason for failure by looking at the information in the IPMI interface for your node/server. These steps are applicable if you are replacing a block drive or a metadata drive.

.What you'll need

* From the NetApp Element software UI, verify that the drive has failed. Element displays an alert when a drive fails. You can access the Element UI by using the management virtual IP (MVIP) address of the primary cluster node.
* Ensure that you have familiarized yourself with all the steps.
* Ensure that you take necessary precautions to prevent electrostatic discharge (ESD) while handling drives.

.Steps

. Remove the failed drive from the cluster as follows using the Element UI:
.. Select *Cluster* > *Drives* > *Failed*.
.. Note the node name and serial number associated with the failed drive.
.. Select the icon under *Actions*, and then select *Remove*.
 If you see warnings of the service associated with the drive, wait until bin sync completes, and then remove the drive.
. Perform the following steps to verify the drive failure and view the events logged that are associated with the drive failure:
.. Log in to the IPMI interface of the node (IDRAC in this case).
.. Select *Maintenance* > *System Event Log* to see the reason for the drive failure (for example, SSDWearOut or drive not inserted properly).
+
You can also see an event showing the status of the drive.
.. Select *Storage* from the menu, and then select *Physical Disks*.
.. Find the slot number of the failed drive using the serial number that you noted in the Element UI.

. Remove the drive physically as follows:
.. Identify the drive slot number in the chassis.
+
The following image shows the front of the server with the slot numbering shown for each drive:
+
image::../media/esds-dell.png[Shows the front of the R640 server with the slot numbering shown for each drive.]

.. Press the button on the drive.
+
The latch comes out.
.. Physically pull the drive out from the slot.
+
NOTE: Ensure that you handle drives very carefully.
. Insert the replacement drive by carefully pushing the drive into the slot using the latch and closing the latch.
+
The drive powers on when inserted correctly.
. Verify the new drive details in iDRAC:
.. Select *Maintenance* > *System Event Log*. You see an event logged for the drive that you added.
.. Select *Storage* from the menu, and then select *Physical Disks*.
.. Verify that the new drive you inserted shows up in the corresponding slot in the UI.
.. To find the serial number of the drive, select the *+* icon next to each PCIe SSD.
. Add the new drive information in the `sf_sds_config.yaml` file for the node in which you replaced the drive.
+
The `sf_sds_config.yaml` file is stored in `/opt/sf/`. This file includes all the information about the drives in the node. Every time you replace a drive, you must enter the replacement drive information in this file. For more information about this file, see link:reference_esds_sf_sds_config_file.html[Contents of the sf_sds_config.yaml file^].
+
.. Establish an SSH connection to the node by using PuTTY.
.. In the PuTTY configuration window, enter node MIP in the *Host Name (or IP address)* field.
.. Select *Open*.
.. In the terminal window that opens, log in with your username and password.
.. Run the `# cat /opt/sf/sf_sds_config.yaml` command to list the contents of the file.
.. Replace the entries in the `dataDevices` or `cacheDevices` lists for the drive you replaced with the new drive information.
.. Run `# systemctl start solidfire-update-drives`.
+
You see the Bash prompt after this command runs. You should go to the Element UI after this to add the drive to the cluster. The Element UI shows an alert for a new drive that is available.

. Select *Cluster* > *Drives* > *Available*.
+
You see the serial number of the new drive that you installed.

. Select the icon under *Actions*, and then select *Add*.
. Refresh the Element UI after the block sync job completes. You see that the alert about the drive available has cleared if you access the *Running Tasks* page from the *Reporting* tab of the Element UI.

=== Replace a cache drive

Perform this procedure if you want to replace the cache drive in your SolidFire eSDS cluster. The cache drive is associated with metadata services. The Element UI *Cluster* > *Drives* page shows the drive wear information.

.What you'll need

* From the NetApp Element software UI, ensure that your cluster is in good health and there are no warnings or cluster faults. You can access the Element UI by using the management virtual IP (MVIP) address of the primary cluster node.
* Ensure that there are no active jobs running on the cluster.
* Ensure that you have familiarized yourself with all the steps.
* Ensure that you remove metadata services from the Element UI.
* Ensure that you take necessary precautions to prevent electrostatic discharge (ESD) while handling drives.

.Steps

. Perform the following steps in the Element UI:
.. In the Element UI, select *Cluster* > *Nodes* > *Active*.
.. Make a note of the node ID and management IP address of the node in which you are replacing the cache drive.
.. If the cache drive is healthy and you are proactively replacing it, select *Active Drives*, locate the metadata drive, and remove it from the UI.
+
After you remove it, the metadata drive goes to *Removing* state first, and then to *Available*.
.. If you are performing replacement after the cache drive failed, the metadata drive will be in *Available* state, and listed under *Cluster* > *Drives* > *Available*.
.. In the Element UI, select *Cluster* > *Drives* > *Active*.
.. Select the metadata drive associated with the NodeName, where you want to do the cache drive replacement.
.. Select *Bulk Actions* > *Remove*. After you remove the drive, the drive goes into the *Removing* state. It stays in the *Removing* state for a while, waiting for the data on the drive to be synced or redistributed to the remaining drives in the cluster. After the remove is complete, the drive moves to the *Available* state.
. Perform the following steps to locate the drive slot of the cache drive that you are replacing:
.. Log in to the IPMI interface of the node (iDRAC in this case).
.. Select *Storage* from the menu, and then select *Physical Disks*.
.. Locate the cache drive.
+
NOTE: Cache drives are of lesser capacity (375 GB) than storage drives, and are PCIe SSDs.
.. Look for the slot number listed for cache drive.
+
This is the physical slot from which you should remove the drive.
. Now that you have identified the drive, physically remove it as follows:
.. Identify the drive slot number in the chassis.
+
The following image shows the front of the server with the slot numbering shown for each drive:
+
image::../media/esds-dell.png[Shows the front of the R640 server with the slot numbering shown for each drive.]

.. Press the button on the drive.
+
The latch comes out.
.. Physically pull the drive out from the slot.
+
NOTE: Ensure that you handle drives very carefully.
+
After you physically remove the drive, the drive state changes to *Failed* in the Element UI.
. Make a note of the model number and the ISN (serial number) of the new cache drive.
. Insert the replacement drive by carefully pushing the drive into the slot using the latch and closing the latch.
+
The drive powers on when inserted correctly.
. Perform the following steps to verify the new drive details in iDRAC:
.. Select *Maintenance* > *System Event Log*. You see an event logged for the drive that you added.
.. Select *Storage* from the menu, and then select *Physical Disks*.
.. Verify that the new drive you inserted shows up in the corresponding slot in the UI.
.. To find the serial number of the drive, select the *+* icon next to each PCIe SSD.
. Add the new cache drive information in the `sf_sds_config.yaml` file for the node in which you replaced the drive.
+
The `sf_sds_config.yaml` file is stored in `/opt/sf/`. This file includes all the information about the drives in the node. Every time you replace a drive, you should enter the replacement drive information in this file. For more information about this file, see link:reference_esds_sf_sds_config_file.html[Contents of the sf_sds_config.yaml file^].

 .. Establish an SSH connection to the node by using PuTTY.
 .. In the PuTTY configuration window, enter node MIP address (that you made a note of from the Element UI earlier) in the *Host Name (or IP address)* field.
 .. Select *Open*.
 .. In the terminal window that opens, log in with your username and password.
 .. Run the `nvme list` command to list the NMVe devices.
+
You can see the model number and serial number of the new cache drive. See the following sample output:
+
image::../media/esds_nvme_list_r640.png[Shows the model number and serial number of the new cache drive.]

 .. Add the new cache drive information in `/opt/sf/sf_sds_config.yaml`.
+
You should replace the existing cache drive model number and serial number with the corresponding information for the new cache drive. See the following example:
+
image::../media/esds_cache_drive_info_r640.png[Shows the model number and serial number.]

 .. Save the `/opt/sf/sf_sds_config.yaml` file.
. Perform the steps for the scenario that is applicable to you:
+
[%header,cols=2*]
|===
|Scenario
|Steps

|The new inserted cache drive shows up after you run the `nvme list` command
a|
. Run `# systemctl restart solidfire`. This takes around three minutes.
. Check the `solidfire` status by running `system status solidfire`.
. Go to step 9.

|The new inserted cache drive does not show up after you run the `nvme list` command
a|
. Reboot the node.
. After the node reboots, verify that the `solidfire` services are running by logging in to the node (using PuTTY), and running the `system status solidfire` command.
. Go to step 9.
|===
+
NOTE: Restarting `solidfire` or rebooting the node causes some cluster faults, which eventually clear in about five minutes.

. In the Element UI, add back the metadata drive that you removed:
.. Select *Cluster* > *Drives* > *Available*.
.. Select the icon under Actions, and select *Add*.
. Refresh your Element UI after the block sync job completes.
+
You can see that the alert about the drive available has cleared along with other cluster faults.

== Find more information
* https://www.netapp.com/data-storage/solidfire/documentation/[NetApp SolidFire Resources Page^]
* https://docs.netapp.com/sfe-122/topic/com.netapp.ndc.sfe-vers/GUID-B1944B0E-B335-4E0B-B9F1-E960BF32AE56.html[Documentation for earlier versions of NetApp SolidFire and Element products^]
